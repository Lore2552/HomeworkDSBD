    \documentclass[11pt]{article} % Aumenta la dimensione del font a 11pt per migliore leggibilità
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage[italian]{babel}
    \usepackage{float}
    
    % Pacchetti per migliorare l'impaginazione
    \usepackage{geometry}
    \geometry{
     a4paper,
     total={170mm,257mm},
     left=20mm,
     top=20mm,
     } % Imposta margini più ampi e definiti (es. 20mm su tutti i lati)
    
    \usepackage{graphicx}
    \usepackage{microtype}
    \usepackage{tabularx}
    \usepackage{url}
    \usepackage{parskip} % Rimuove l'indentazione e usa spazio verticale tra i paragrafi
    \usepackage{booktabs} % Per tabelle professionali con linee orizzontali eleganti
    \usepackage{makecell} % Per andare a capo nelle celle delle tabelle
    % Pacchetto per personalizzare l'indice (opzionale, ma consigliato per i documenti più lunghi)
    \usepackage{tocloft} 
    
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}
    
    \title{Relazione Homework 2 DSBD}
    \author{Lorenzo Varsallona (1000084765) - Paola Pappalardo (1000035439)}
    \date{}
    
\begin{document}
\maketitle
\tableofcontents 
\newpage % Inizia la prima sezione su una nuova pagina per chiarezza
\section{Introduzione e obiettivi}
Il presente documento illustra le modifiche apportate e le decisioni progettuali prese per la terza e ultima parte dell'homework.
Quest'ultima parte del lavoro, si è focalizzata sull'introduzione di un sistema di metriche e logs tramite prometheus, e una completa
migrazione verso l'utilizzo di kubernetes tramite Kind.
Nelle sezioni successive, si approfondiranno nel dettaglio le funzionalità introdotte e le decisioni progettuali.
\section{Architettura del sistema e decomposizione}
    
    In questa sezione verranno analizzati gli aspetti implementativi dell'applicativo, con particolare attenzione all'evoluzione architetturale introdotta per soddisfare i requisiti della terza parte dell'homework. L'obiettivo principale è stato il passaggio da un'orchestrazione basata su Docker Compose a un deployment completo su \textbf{Kubernetes}, integrando al contempo un sistema avanzato di monitoraggio.
    
    \begin{figure}[H] 
        \centering 
        \includegraphics[width=0.8\textwidth]{schema_architetturale.png} 
        \caption{Diagramma architetturale generale del sistema aggiornato.} 
        \label{fig:architettura} 
    \end{figure}
    
    \subsection{Evoluzione dell'Architettura}
    
    L'applicativo mantiene la sua struttura a microservizi, consolidata nelle fasi precedenti, ma viene ora eseguito interamente all'interno di un cluster Kubernetes (gestito localmente tramite \textbf{Kind}). 
    
\subsection{Monitoraggio e Observability}
    
    Una delle novità principali di questa fase è l'introduzione di \textbf{Prometheus} per il monitoraggio attivo del cluster. Ogni microservizio è stato strumentato per esporre metriche personalizzate, che vengono raccolte periodicamente da Prometheus. Questo permette di avere una visione in tempo reale dello stato di salute e delle performance dell'intero sistema distribuito.
    
    Infine, l'accesso ai servizi dall'esterno del cluster è gestito tramite un \textbf{Ingress Controller NGINX}, configurato per instradare il traffico HTTP e gRPC verso i corretti Service Kubernetes, garantendo un unico punto di ingresso sicuro e gestibile.

    \subsection{Dettaglio delle Metriche Prometheus}
    Per monitorare efficacemente lo stato del sistema, sono state definite metriche specifiche per i microservizi principali, suddivise in \textit{Counter} (contatori incrementali) e \textit{Gauge} (valori istantanei).
    
    \subsubsection{User-Manager}
    Il servizio di gestione utenti espone le seguenti metriche:
    \begin{itemize}
        \item \textbf{Counter}:
        \begin{itemize}
            \item \texttt{messages\_cleaned\_total}: conta il numero totale di messaggi scaduti rimossi dal sistema presenti nella cache.
            \item \texttt{user\_registration\_errors\_total}: traccia il numero totale di errori verificatisi durante la registrazione degli utenti, classificati per tipologia.
        \end{itemize}
        \item \textbf{Gauge}:
        \begin{itemize}
            \item \texttt{cleanup\_duration\_seconds}: misura il tempo impiegato per l'operazione di pulizia dei messaggi scaduti.
        \end{itemize}
    \end{itemize}
    
    \subsubsection{Data-Collector}
    Il servizio di raccolta dati sui voli espone le seguenti metriche:
    \begin{itemize}
        \item \textbf{Counter}:
        \begin{itemize}
            \item \texttt{flights\_collected\_total}: conta il numero totale di voli raccolti e processati.
            \item \texttt{collection\_errors\_total}: traccia il numero totale di errori durante la fase di raccolta dati.
        \end{itemize}
        \item \textbf{Gauge}:
        \begin{itemize}
            \item \texttt{flight\_collection\_duration\_seconds}: misura la durata di ogni ciclo di raccolta dei dati di volo, permettendo di monitorare la latenza delle operazioni esterne.
        \end{itemize}
    \end{itemize}

    \section{Deployment e Configurazione Kubernetes}
    Il deployment dell'infrastruttura è stato definito attraverso un insieme di manifest YAML dichiarativi, ciascuno responsabile della configurazione di uno specifico componente del sistema. Di seguito viene analizzata la configurazione adottata per ogni microservizio e risorsa.

    \subsection{Analisi dei Manifest}
    \subsubsection{Database PostgreSQL (\texttt{postgres.yaml})}
    Il database relazionale è configurato utilizzando:
    \begin{itemize}
        \item \textbf{Deployment}: gestisce l'istanza del database.
        \item \textbf{PersistentVolumeClaim (PVC)}: garantisce la persistenza dei dati anche in caso di riavvio del pod.
        \item \textbf{ConfigMap}: monta lo script \texttt{init.sql} per l'inizializzazione automatica dello schema all'avvio.
        \item \textbf{Secret}: le credenziali di accesso sono iniettate in modo sicuro tramite variabili d'ambiente referenziate dal file \texttt{secrets.yaml}.
        \item \textbf{Service}: di tipo \textit{ClusterIP}, rende il database raggiungibile dagli altri pod tramite il nome DNS interno \texttt{postgres}.
    \end{itemize}

    \subsubsection{Apache Kafka (\texttt{kafka.yaml})}
    Il message broker è stato configurato per operare in modalità KRaft (senza Zookeeper):
    \begin{itemize}
        \item \textbf{Deployment}: configura il broker Kafka. Un aspetto critico è stato l'impostazione di \texttt{enableServiceLinks: false}. Questa direttiva impedisce a Kubernetes di iniettare variabili d'ambiente di servizio (es. \texttt{KAFKA\_PORT}) che entravano in conflitto con le variabili di configurazione interne dell'immagine Docker, causando errori di avvio.
        \item \textbf{Service}: di tipo \textit{ClusterIP}, espone la porta 9092 per la comunicazione interna tra i microservizi produttori e consumatori.
    \end{itemize}

    \subsubsection{Microservizi Core (\texttt{user-manager.yaml}, \texttt{data-collector.yaml})}
    I due servizi principali condividono una struttura simile:
    \begin{itemize}
        \item \textbf{Deployment}: definisce le repliche e le risorse. Le variabili d'ambiente sensibili (es. stringhe di connessione DB, chiavi API) sono caricate referenziando i \textbf{Secret}.
        \item \textbf{Service}: di tipo \textit{ClusterIP}, espongono le porte necessarie per la comunicazione gRPC (50051) e HTTP (5000), oltre alla porta per le metriche Prometheus.
    \end{itemize}

    \subsubsection{Sistemi di Alerting (\texttt{alert-system.yaml}, \texttt{alert-notifier-system.yaml})}
    Questi componenti agiscono come \textit{worker} puri (consumatori Kafka):
    \begin{itemize}
        \item \textbf{Deployment}: gestisce il ciclo di vita dei pod.
        \item \textbf{Assenza di Service}: non dovendo ricevere traffico diretto in ingresso (né HTTP né gRPC), non dispongono di un oggetto Service associato. Comunicano esclusivamente in uscita verso Kafka e, nel caso del notifier, verso server SMTP esterni (quelli forniti da gmail).
    \end{itemize}

    \subsubsection{Prometheus (\texttt{prometheus.yaml})}
    Il sistema di monitoraggio è configurato con:
        \begin{itemize}
        \item \textbf{ConfigMap}: contiene il file \texttt{prometheus.yml}, che definisce i target di scraping (i servizi da monitorare) e gli intervalli di aggiornamento.
        \item \textbf{Deployment}: esegue il server Prometheus.
        \item \textbf{Service}: di tipo \textit{NodePort}, espone la dashboard di Prometheus sulla porta \texttt{30090}, rendendola accessibile direttamente dall'host, oltre che tramite il proxy NGINX.
    \end{itemize}

    \subsubsection{Ingress Gateway (\texttt{nginx.yaml})}
    NGINX funge da punto di ingresso unico per il cluster:
    \begin{itemize}
        \item \textbf{Service NodePort}: A differenza degli altri servizi, NGINX utilizza un Service di tipo \textbf{NodePort} esposto sulle porte \texttt{30080} (HTTP) e \texttt{30443} (HTTPS). Questa scelta è stata preferita al semplice \textit{port-forwarding} per garantire un accesso stabile e persistente all'applicazione dall'host di sviluppo, simulando il comportamento di un LoadBalancer di produzione.
        \item \textbf{ConfigMap}: Inietta il file \texttt{nginx.conf} che definisce le regole di reverse proxy per instradare il traffico verso i servizi backend e verso la dashboard di Prometheus.
        \item \textbf{Secret}: Gestisce i certificati TLS per la terminazione SSL.
    \end{itemize}

    \subsubsection{Gestione dei secrets: (\texttt{secrets.yaml})}
    Tutte le informazioni sensibili (password DB, API Key OpenSky, credenziali email,ecc) sono centralizzate in questo manifest. I valori sono codificati in \textbf{Base64} e montati come variabili d'ambiente nei container, garantendo che nessuna credenziale appaia in chiaro nelle configurazioni dei Deployment.
\end{document}
